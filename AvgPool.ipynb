{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms \n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from Dataset import YTBDatasetVer,YTBDatasetCNN\n",
    "from Network import NANNet,CNNNet\n",
    "import numpy as np\n",
    "from util import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1' # 设置跑第几个GPU\n",
    "# 使用cuda运算\n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC曲线绘制函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fpr, tpr, figure_name=\"roc.png\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig = plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    fig.savefig(os.path.join(\"./\", figure_name), dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YTBDatasetVer(csv_file='../splits.txt', root_dir='../aligned_images_DB', img_size=224,num_frames=100)\n",
    "dataload = torch.utils.data.DataLoader(dataset, shuffle=True, batch_size=10, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化萌新（模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pretrained weight successfully\n"
     ]
    }
   ],
   "source": [
    "model=NANNet(cnn_path='./checkpoints/cnn_modelacc0.9982.pth',ave_pool=True).to(device)\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看可以更新的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention.q\n",
      "attention.fc.weight\n",
      "attention.fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取存储好的NAN模型权值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"nan_model_bat.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b093bedeb1674808b42cf3f6f95e7050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[91mTrain set: Accuracy: 0.65911111\n",
      "\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c28257e38b4b1a93400d434ba933a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/lulu/Dataset/face_rec/Dataset.py\", line 178, in __getitem__\n",
      "    data_face1 = self.load_face_from_dir(face_dir)\n",
      "  File \"/home/lulu/Dataset/face_rec/Dataset.py\", line 138, in load_face_from_dir\n",
      "    img = self.rgb_transform(img)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 91, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 90, in to_tensor\n",
      "    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n",
      "KeyboardInterrupt\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/lulu/Dataset/face_rec/Dataset.py\", line 178, in __getitem__\n",
      "    data_face1 = self.load_face_from_dir(face_dir)\n",
      "  File \"/home/lulu/Dataset/face_rec/Dataset.py\", line 138, in load_face_from_dir\n",
      "    img = self.rgb_transform(img)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 235, in __call__\n",
      "    return F.center_crop(img, self.size)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 369, in center_crop\n",
      "    return crop(img, i, j, th, tw)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\", line 359, in crop\n",
      "    return img.crop((j, i, j + w, i + h))\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1136, in crop\n",
      "    self.load()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 242, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f45e6601e48>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 1033) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e7c443e7274e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtotal_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.init_weights()\n",
    "acc_max = 0\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(),lr=0.05)\n",
    "# optimizer =torch.optim.RMSprop(model.parameters(),lr=0.001, weight_decay=1e-5)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(),lr=0.05)\n",
    "# optimizer=torch.optim.Adagrad(model.parameters(), lr=0.0005, lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n",
    "for epoch in range(300):\n",
    "    total_loss = 0\n",
    "    total_size = 0\n",
    "    bar = tqdm(dataload)\n",
    "    labels, distances = [], []\n",
    "    for i, (x1, x2, y) in enumerate(bar):\n",
    "#         optimizer.zero_grad()\n",
    "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "        _,_,l2 = model(x1, x2)\n",
    "        total_size += l2.size(0)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        # b=pred.item()\n",
    "        distances.append(l2.detach().data.cpu().numpy())\n",
    "        labels.append(y.cpu().numpy())\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "        bar.set_postfix(epoch=f\"{epoch+1}\")\n",
    "    labels = np.concatenate(labels)\n",
    "    distances = np.concatenate(distances)\n",
    "\n",
    "    tpr, fpr, accuracy, val, val_std, far = evaluate(distances, labels)\n",
    "    print('\\33[91mTrain set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    plot_roc(fpr, tpr, figure_name=\"roc_train_epoch_{}.png\".format(epoch))\n",
    "\n",
    "    acc = np.mean(accuracy)\n",
    "    torch.save(model.state_dict(), \"nan_model.pth\")\n",
    "#     if acc_max < acc:\n",
    "#         acc_max = max(acc, acc_max)\n",
    "#         torch.save(model.state_dict(), f\"./checkpoints/nan_model_acc{acc_max:0.4f}.pth\")\n",
    "#     if acc>0.8:\n",
    "#         optimizer = torch.optim.Adadelta(model.parameters(),lr=1-acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试人脸验证"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
